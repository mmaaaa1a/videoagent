import os
import logging
import time
from tqdm import tqdm
from faster_whisper import WhisperModel

def speech_to_text(video_name, working_dir, segment_index2name, audio_output_format, use_epyc_optimization=False):
    """
    è¯­éŸ³è¯†åˆ«å‡½æ•°ï¼Œæ”¯æŒä¼ ç»Ÿæ¨¡å¼å’ŒEPYCä¼˜åŒ–æ¨¡å¼

    Args:
        video_name: è§†é¢‘åç§°
        working_dir: å·¥ä½œç›®å½•
        segment_index2name: ç‰‡æ®µç´¢å¼•æ˜ å°„
        audio_output_format: éŸ³é¢‘æ ¼å¼
        use_epyc_optimization: æ˜¯å¦ä½¿ç”¨EPYCä¼˜åŒ–æ¨¡å¼
    """
    if use_epyc_optimization:
        try:
            # å°è¯•å¯¼å…¥EPYCä¼˜åŒ–æ¨¡å—
            from .asr_epyc_optimized import speech_to_text_epyc_optimized
            print("ğŸš€ ä½¿ç”¨EPYC 64æ ¸å¿ƒä¼˜åŒ–æ¨¡å¼")
            return speech_to_text_epyc_optimized(video_name, working_dir, segment_index2name, audio_output_format)
        except ImportError as e:
            print(f"âš ï¸ EPYCä¼˜åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼: {e}")
            use_epyc_optimization = False

    # ä¼ ç»Ÿæ¨¡å¼å¤„ç†
    print("ğŸ“ ä½¿ç”¨ä¼ ç»Ÿè¯­éŸ³è¯†åˆ«æ¨¡å¼")
    start_time = time.time()

    # ä½¿ç”¨faster-whisperå†…ç½®æ¨¡å‹ï¼Œfaster-distil-whisper-large-v3ç­‰æ•ˆäºlarge-v3
    model = WhisperModel("large-v3", device="cpu")
    model.logger.setLevel(logging.WARNING)

    cache_path = os.path.join(working_dir, '_cache', video_name)

    transcripts = {}
    for index in tqdm(segment_index2name, desc=f"Speech Recognition {video_name}"):
        segment_name = segment_index2name[index]
        audio_file = os.path.join(cache_path, f"{segment_name}.{audio_output_format}")

        # if the audio file does not exist, skip it
        if not os.path.exists(audio_file):
            transcripts[index] = ""
            continue

        segments, info = model.transcribe(audio_file)

        # å¤„ç†transcribeå¯èƒ½è¿”å›Noneçš„æƒ…å†µ
        if segments is None:
            transcripts[index] = ""
            continue

        result = ""
        for segment in segments:
            result += "[%.2fs -> %.2fs] %s\n" % (segment.start, segment.end, segment.text)
        transcripts[index] = result

    # æ€§èƒ½ç»Ÿè®¡
    elapsed_time = time.time() - start_time
    avg_time_per_file = elapsed_time / len(segment_index2name) if segment_index2name else 0

    print(f"âœ… ä¼ ç»Ÿæ¨¡å¼è¯­éŸ³è¯†åˆ«å®Œæˆ:")
    print(f"   - å¤„ç†æ–‡ä»¶æ•°: {len(segment_index2name)}")
    print(f"   - æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    print(f"   - å¹³å‡æ¯æ–‡ä»¶: {avg_time_per_file:.2f}ç§’")
    print(f"   - å¤„ç†é€Ÿåº¦: {len(segment_index2name)/elapsed_time:.2f}æ–‡ä»¶/ç§’")

    return transcripts