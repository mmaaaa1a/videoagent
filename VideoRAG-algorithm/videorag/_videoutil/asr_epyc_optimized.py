import os
import sys
import time
import logging
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from tqdm import tqdm
from faster_whisper import WhisperModel
from typing import List, Tuple, Dict, Any
from .epyc_config import setup_epyc_environment

# è®¾ç½®EPYCå¤„ç†å™¨ä¼˜åŒ–ç¯å¢ƒå˜é‡
def setup_epyc_optimization():
    """é…ç½®EPYC 64æ ¸å¿ƒå¤„ç†å™¨çš„æ€§èƒ½ä¼˜åŒ–"""
    config = setup_epyc_environment()

    # PyTorchä¼˜åŒ–
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'

    # è®¾ç½®æ—¥å¿—çº§åˆ«å‡å°‘è¾“å‡ºå¼€é”€
    logging.getLogger("faster_whisper").setLevel(logging.WARNING)
    logging.getLogger("ctranslate2").setLevel(logging.WARNING)

    return config

class ModelCache:
    """æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨ï¼Œåˆ©ç”¨256GBå¤§å†…å­˜"""
    def __init__(self, max_cache_size: int = 5):
        self.cache = {}
        self.max_cache_size = max_cache_size
        self.cache_hits = 0
        self.cache_misses = 0

    def get_model(self, model_name: str = "large-v3", device: str = "cpu", compute_type: str = "float32"):
        cache_key = f"{model_name}_{device}_{compute_type}"

        if cache_key in self.cache:
            self.cache_hits += 1
            return self.cache[cache_key]

        if len(self.cache) >= self.max_cache_size:
            # ç§»é™¤æœ€æ—§çš„ç¼“å­˜
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache_misses += 1
        model = WhisperModel(model_name, device=device, compute_type=compute_type)
        model.logger.setLevel(logging.WARNING)
        self.cache[cache_key] = model
        return model

    def get_cache_stats(self) -> Dict[str, Any]:
        total = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total if total > 0 else 0
        return {
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": f"{hit_rate:.2%}",
            "cached_models": len(self.cache)
        }

class EPYCWhisperManager:
    """EPYC 64æ ¸å¿ƒä¼˜åŒ–çš„Whisperç®¡ç†å™¨"""

    def __init__(self, config_override: Dict[str, Any] = None):
        # è·å–EPYCé…ç½®
        self.config = setup_epyc_optimization()

        # å…è®¸é…ç½®è¦†ç›–
        if config_override:
            self.config.update(config_override)

        self.num_models = self.config.get('num_models', 8)
        self.batch_size = self.config.get('batch_size', 32)
        self.workers = self.config.get('workers', 64)
        self.compute_type = self.config.get('compute_type', 'float32')

        self.model_cache = ModelCache(max_cache_size=self.config.get('model_cache_size', 5))

        # é¢„åŠ è½½æ¨¡å‹åˆ°å†…å­˜
        print(f"ğŸš€ åˆå§‹åŒ–EPYCä¼˜åŒ–Whisperç®¡ç†å™¨:")
        print(f"   - æ¨¡å‹å®ä¾‹æ•°: {self.num_models}")
        print(f"   - æ‰¹é‡å¤§å°: {self.batch_size}")
        print(f"   - å·¥ä½œè¿›ç¨‹æ•°: {self.workers}")
        print(f"   - è®¡ç®—ç±»å‹: {self.compute_type}")

        self.models = []
        for i in range(self.num_models):
            model = self.model_cache.get_model("large-v3", "cpu", self.compute_type)
            self.models.append(model)

        print(f"âœ… EPYC Whisperç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå†…å­˜ç¼“å­˜: {self.model_cache.get_cache_stats()}")

    def _process_single_file(self, model_idx: int, audio_file: str, seg_index: str) -> Tuple[str, str, str]:
        """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        if not os.path.exists(audio_file):
            return (audio_file, seg_index, "")

        try:
            # è·å–æ¨¡å‹å®ä¾‹ï¼ˆè½®æ¢ä½¿ç”¨ï¼‰
            model = self.models[model_idx % len(self.models)]

            # ä¼˜åŒ–å‚æ•°è®¾ç½®
            segments, info = model.transcribe(
                audio_file,
                beam_size=2,  # å‡å°‘beam sizeæå‡é€Ÿåº¦
                language=None,  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                condition_on_previous_text=False,  # å‡å°‘ä¾èµ–
                vad_filter=False  # ä¸ä½¿ç”¨VADä»¥æå‡é€Ÿåº¦
            )

            result = ""
            for segment in segments:
                result += "[%.2fs -> %.2fs] %s\n" % (segment.start, segment.end, segment.text)

            return (audio_file, seg_index, result)

        except Exception as e:
            logging.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ {audio_file}: {e}")
            return (audio_file, seg_index, "")

    def _process_batch_threaded(self, model_idx: int, batch_files: List[Tuple[str, str]]) -> List[Tuple[str, str, str]]:
        """ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        results = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†æ‰¹æ¬¡å†…çš„æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=min(16, len(batch_files))) as executor:
            # æäº¤æ‰¹æ¬¡å†…çš„æ‰€æœ‰æ–‡ä»¶
            future_to_file = {}
            for audio_file, seg_index in batch_files:
                future = executor.submit(self._process_single_file, model_idx, audio_file, seg_index)
                future_to_file[future] = (audio_file, seg_index)

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_file):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    audio_file, seg_index = future_to_file[future]
                    logging.error(f"æ‰¹æ¬¡å†…æ–‡ä»¶å¤„ç†å¤±è´¥ {audio_file}: {e}")
                    results.append((audio_file, seg_index, ""))

        return results

    def parallel_transcribe(self, audio_files: List[Tuple[str, str]]) -> List[Tuple[str, str, str]]:
        """å¹¶è¡Œè½¬å½•éŸ³é¢‘æ–‡ä»¶ï¼Œå……åˆ†åˆ©ç”¨64æ ¸å¿ƒ"""
        if not audio_files:
            return []

        start_time = time.time()

        # åˆ†æ‰¹å¤„ç†
        batches = []
        for i in range(0, len(audio_files), self.batch_size):
            batch = audio_files[i:i+self.batch_size]
            model_idx = (i // self.batch_size) % self.num_models
            batches.append((model_idx, batch))

        all_results = []

        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        with ProcessPoolExecutor(max_workers=min(self.workers, len(batches))) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {}
            for model_idx, batch in batches:
                future = executor.submit(self._process_batch_threaded, model_idx, batch)
                future_to_batch[future] = batch

            # æ”¶é›†ç»“æœ
            with tqdm(total=len(audio_files), desc="ğŸ¤ EPYCæ‰¹é‡è¯­éŸ³è¯†åˆ«") as pbar:
                for future in as_completed(future_to_batch):
                    try:
                        batch_results = future.result()
                        all_results.extend(batch_results)
                        pbar.update(len(batch_results))
                    except Exception as e:
                        logging.error(f"æ‰¹æ¬¡æ‰§è¡Œå¤±è´¥: {e}")
                        batch = future_to_batch[future]
                        # é™çº§å¤„ç†å¤±è´¥æ‰¹æ¬¡
                        for audio_file, seg_index in batch:
                            all_results.append((audio_file, seg_index, ""))
                        pbar.update(len(batch))

        elapsed_time = time.time() - start_time
        files_per_second = len(audio_files) / elapsed_time if elapsed_time > 0 else 0

        print(f"ğŸ‰ EPYCå¹¶è¡Œè½¬å½•å®Œæˆ: {len(audio_files)}ä¸ªæ–‡ä»¶, è€—æ—¶: {elapsed_time:.2f}ç§’, é€Ÿåº¦: {files_per_second:.2f}æ–‡ä»¶/ç§’")
        print(f"ğŸ“Š æ¨¡å‹ç¼“å­˜ç»Ÿè®¡: {self.model_cache.get_cache_stats()}")

        return all_results

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "num_models": self.num_models,
            "batch_size": self.batch_size,
            "cache_stats": self.model_cache.get_cache_stats(),
            "environment": {
                "OMP_NUM_THREADS": os.environ.get('OMP_NUM_THREADS'),
                "MKL_NUM_THREADS": os.environ.get('MKL_NUM_THREADS'),
                "OPENBLAS_NUM_THREADS": os.environ.get('OPENBLAS_NUM_THREADS')
            }
        }

# å…¨å±€EPYCç®¡ç†å™¨å®ä¾‹
_epyc_manager = None

def get_epyc_manager(config_override: Dict[str, Any] = None) -> EPYCWhisperManager:
    """è·å–å…¨å±€EPYC Whisperç®¡ç†å™¨å®ä¾‹"""
    global _epyc_manager
    if _epyc_manager is None:
        _epyc_manager = EPYCWhisperManager(config_override=config_override)
    return _epyc_manager

def speech_to_text_epyc_optimized(video_name: str, working_dir: str, segment_index2name: Dict[str, str], audio_output_format: str) -> Dict[str, str]:
    """
    EPYC 64æ ¸å¿ƒä¼˜åŒ–çš„è¯­éŸ³è¯†åˆ«å‡½æ•°

    Args:
        video_name: è§†é¢‘åç§°
        working_dir: å·¥ä½œç›®å½•
        segment_index2name: ç‰‡æ®µç´¢å¼•åˆ°åç§°çš„æ˜ å°„
        audio_output_format: éŸ³é¢‘è¾“å‡ºæ ¼å¼

    Returns:
        è½¬å½•ç»“æœå­—å…¸ {segment_index: transcript}
    """
    print(f"ğŸš€ å¼€å§‹EPYCä¼˜åŒ–è¯­éŸ³è¯†åˆ«: {video_name}")
    start_time = time.time()

    cache_path = os.path.join(working_dir, '_cache', video_name)

    # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    segment_indices = []

    for index in segment_index2name:
        segment_name = segment_index2name[index]
        audio_file = os.path.join(cache_path, f"{segment_name}.{audio_output_format}")

        if os.path.exists(audio_file):
            audio_files.append((audio_file, index))
            segment_indices.append(index)
        else:
            logging.warning(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")

    if not audio_files:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶")
        return {}

    print(f"ğŸ“ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶å¾…å¤„ç†")

    # è·å–EPYCç®¡ç†å™¨å¹¶å¹¶è¡Œå¤„ç†
    manager = get_epyc_manager()
    results = manager.parallel_transcribe(audio_files)

    # æ„å»ºè¿”å›ç»“æœ
    transcripts = {}
    for audio_file, seg_index, result in results:
        transcripts[seg_index] = result

    # æ€§èƒ½ç»Ÿè®¡
    total_time = time.time() - start_time
    avg_time_per_file = total_time / len(audio_files) if audio_files else 0

    print(f"âœ… EPYCä¼˜åŒ–è¯­éŸ³è¯†åˆ«å®Œæˆ:")
    print(f"   - å¤„ç†æ–‡ä»¶æ•°: {len(audio_files)}")
    print(f"   - æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"   - å¹³å‡æ¯æ–‡ä»¶: {avg_time_per_file:.2f}ç§’")
    print(f"   - å¤„ç†é€Ÿåº¦: {len(audio_files)/total_time:.2f}æ–‡ä»¶/ç§’")

    return transcripts