import os
import sys
import time
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from faster_whisper import WhisperModel
from typing import List, Tuple, Dict, Any
from .epyc_config import setup_epyc_environment
import psutil
from .._storage import IntermediateStorageManager

# è®¾ç½®EPYCå¤„ç†å™¨ä¼˜åŒ–ç¯å¢ƒå˜é‡
def setup_epyc_optimization():
    """é…ç½®EPYC 64æ ¸å¿ƒå¤„ç†å™¨çš„æ€§èƒ½ä¼˜åŒ–"""
    config = setup_epyc_environment()

    # PyTorchä¼˜åŒ–
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'

    # è®¾ç½®æ—¥å¿—çº§åˆ«å‡å°‘è¾“å‡ºå¼€é”€
    logging.getLogger("faster_whisper").setLevel(logging.WARNING)
    logging.getLogger("ctranslate2").setLevel(logging.WARNING)

    return config

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    def __init__(self):
        self.start_time = time.time()
        self.processed_files = 0
        self.total_files = 0
        self.cpu_usage_history = []
        self.memory_usage_history = []
        self.lock = threading.Lock()

    def update_progress(self, processed_count: int = 1):
        """æ›´æ–°å¤„ç†è¿›åº¦"""
        with self.lock:
            self.processed_files += processed_count
            # è®°å½•ç³»ç»Ÿæ€§èƒ½
            self.cpu_usage_history.append(psutil.cpu_percent())
            self.memory_usage_history.append(psutil.virtual_memory().percent)

    def get_current_stats(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            elapsed_time = time.time() - self.start_time
            files_per_second = self.processed_files / elapsed_time if elapsed_time > 0 else 0
            avg_cpu = sum(self.cpu_usage_history) / len(self.cpu_usage_history) if self.cpu_usage_history else 0
            avg_memory = sum(self.memory_usage_history) / len(self.memory_usage_history) if self.memory_usage_history else 0

            return {
                "elapsed_time": elapsed_time,
                "processed_files": self.processed_files,
                "total_files": self.total_files,
                "files_per_second": files_per_second,
                "progress_percent": (self.processed_files / self.total_files * 100) if self.total_files > 0 else 0,
                "current_cpu_usage": psutil.cpu_percent(),
                "current_memory_usage": psutil.virtual_memory().percent,
                "avg_cpu_usage": avg_cpu,
                "avg_memory_usage": avg_memory,
                "eta_seconds": (self.total_files - self.processed_files) / files_per_second if files_per_second > 0 else 0,
                "cpu_usage_history": self.cpu_usage_history.copy(),
                "memory_usage_history": self.memory_usage_history.copy()
            }

class ModelCache:
    """ä¼˜åŒ–çš„æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨"""
    def __init__(self, max_cache_size: int = 3):  # å‡å°‘ç¼“å­˜å¤§å°ï¼Œé¿å…å†…å­˜è¿‡åº¦å ç”¨
        self.cache = {}
        self.max_cache_size = max_cache_size
        self.cache_hits = 0
        self.cache_misses = 0
        self.lock = threading.Lock()

    def get_model(self, model_name: str = "large-v3", device: str = "cpu", compute_type: str = "float32"):
        cache_key = f"{model_name}_{device}_{compute_type}"

        with self.lock:
            if cache_key in self.cache:
                self.cache_hits += 1
                return self.cache[cache_key]

            if len(self.cache) >= self.max_cache_size:
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]

            self.cache_misses += 1
            model = WhisperModel(model_name, device=device, compute_type=compute_type)
            model.logger.setLevel(logging.WARNING)
            self.cache[cache_key] = model
            return model

    def get_cache_stats(self) -> Dict[str, Any]:
        with self.lock:
            total = self.cache_hits + self.cache_misses
            hit_rate = self.cache_hits / total if total > 0 else 0
            return {
                "cache_hits": self.cache_hits,
                "cache_misses": self.cache_misses,
                "hit_rate": f"{hit_rate:.2%}",
                "cached_models": len(self.cache)
            }

class EPYCWhisperManager:
    """é‡æ„çš„EPYC 64æ ¸å¿ƒä¼˜åŒ–Whisperç®¡ç†å™¨"""

    def __init__(self, config_override: Dict[str, Any] = None):
        # è·å–EPYCé…ç½®
        self.config = setup_epyc_optimization()

        # å…è®¸é…ç½®è¦†ç›–
        if config_override:
            self.config.update(config_override)

        self.num_models = self.config.get('num_models', 8)
        self.batch_size = self.config.get('batch_size', 16)  # å‡å°‘æ‰¹é‡å¤§å°
        self.workers = self.config.get('workers', 32)       # å‡å°‘å·¥ä½œè¿›ç¨‹æ•°ï¼Œé¿å…è¿‡åº¦ç«äº‰
        self.compute_type = self.config.get('compute_type', 'float32')

        self.model_cache = ModelCache(max_cache_size=self.config.get('model_cache_size', 3))
        self.performance_monitor = PerformanceMonitor()

        # é¢„åŠ è½½æ¨¡å‹åˆ°å†…å­˜
        print(f"ğŸš€ åˆå§‹åŒ–é‡æ„ç‰ˆEPYCä¼˜åŒ–Whisperç®¡ç†å™¨:")
        print(f"   - æ¨¡å‹å®ä¾‹æ•°: {self.num_models}")
        print(f"   - æ‰¹é‡å¤§å°: {self.batch_size}")
        print(f"   - å·¥ä½œè¿›ç¨‹æ•°: {self.workers}")
        print(f"   - è®¡ç®—ç±»å‹: {self.compute_type}")
        print(f"   - ç³»ç»Ÿæ ¸å¿ƒæ•°: {psutil.cpu_count()}")
        print(f"   - ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory().total // (1024**3)}GB")

        self.models = []
        for i in range(self.num_models):
            model = self.model_cache.get_model("large-v3", "cpu", self.compute_type)
            self.models.append(model)

        print(f"âœ… EPYC Whisperç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå†…å­˜ç¼“å­˜: {self.model_cache.get_cache_stats()}")

    def _process_single_file(self, args: Tuple[str, str, int]) -> Tuple[str, str, str]:
        """ä¼˜åŒ–çš„å•æ–‡ä»¶å¤„ç†å‡½æ•°"""
        audio_file, seg_index, model_idx = args

        if not os.path.exists(audio_file):
            return (audio_file, seg_index, "")

        try:
            # è·å–æ¨¡å‹å®ä¾‹ï¼ˆè½®æ¢ä½¿ç”¨ï¼‰
            model = self.models[model_idx % len(self.models)]

            # ä¼˜åŒ–å‚æ•°è®¾ç½® - æå‡é€Ÿåº¦
            segments, info = model.transcribe(
                audio_file,
                beam_size=1,                              # æœ€å°beam sizeæœ€å¤§åŒ–é€Ÿåº¦
                language=None,                            # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                condition_on_previous_text=False,         # å‡å°‘ä¾èµ–
                vad_filter=False,                         # ä¸ä½¿ç”¨VADä»¥æå‡é€Ÿåº¦
                word_timestamps=False,                    # ä¸éœ€è¦è¯çº§æ—¶é—´æˆ³
                temperature=0.0                           # ç¡®å®šæ€§è¾“å‡º
            )

            result = ""
            for segment in segments:
                result += "[%.2fs -> %.2fs] %s\n" % (segment.start, segment.end, segment.text)

            return (audio_file, seg_index, result)

        except Exception as e:
            logging.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ {audio_file}: {e}")
            return (audio_file, seg_index, "")

    def parallel_transcribe(self, audio_files: List[Tuple[str, str]], progress_callback=None) -> List[Tuple[str, str, str]]:
        """é‡æ„çš„å¹¶è¡Œè½¬å½•å‡½æ•° - ç®€åŒ–æ¶æ„ï¼Œæå‡æ€§èƒ½"""
        if not audio_files:
            return []

        start_time = time.time()

        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
        self.performance_monitor.total_files = len(audio_files)
        self.performance_monitor.processed_files = 0

        print(f"ğŸ“ å¼€å§‹å¤„ç† {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        print(f"ğŸ¯ é…ç½®: æ¨¡å‹æ•°={self.num_models}, æ‰¹é‡å¤§å°={self.batch_size}, å·¥ä½œè¿›ç¨‹æ•°={self.workers}")

        # å‡†å¤‡ä»»åŠ¡å‚æ•° - é¿å…åµŒå¥—çº¿ç¨‹æ± 
        task_args = []
        for i, (audio_file, seg_index) in enumerate(audio_files):
            model_idx = i % self.num_models  # ç®€å•çš„è½®æ¢ç­–ç•¥
            task_args.append((audio_file, seg_index, model_idx))

        all_results = []

        # ä½¿ç”¨å•ä¸€çº¿ç¨‹æ± ç›´æ¥å¤„ç†æ‰€æœ‰æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {}
            for i, args in enumerate(task_args):
                future = executor.submit(self._process_single_file, args)
                future_to_index[future] = i

            # åˆ›å»ºå¢å¼ºçš„è¿›åº¦æ¡
            with tqdm(total=len(audio_files), desc="ğŸ¤ EPYCä¼˜åŒ–è¯­éŸ³è¯†åˆ«",
                     unit="æ–‡ä»¶", unit_scale=False, dynamic_ncols=True) as pbar:

                # å®šæœŸæ›´æ–°æ€§èƒ½ä¿¡æ¯
                last_stats_time = time.time()

                for future in as_completed(future_to_index):
                    try:
                        result = future.result()
                        all_results.append(result)

                        # æ›´æ–°è¿›åº¦å’Œæ€§èƒ½ç›‘æ§
                        self.performance_monitor.update_progress(1)
                        pbar.update(1)

                        # æ¯2ç§’æ›´æ–°ä¸€æ¬¡è¯¦ç»†æ€§èƒ½ä¿¡æ¯
                        current_time = time.time()
                        if current_time - last_stats_time >= 2.0:
                            stats = self.performance_monitor.get_current_stats()
                            pbar.set_postfix({
                                'é€Ÿåº¦': f"{stats['files_per_second']:.2f}/s",
                                'CPU': f"{stats['current_cpu_usage']:.0f}%",
                                'å†…å­˜': f"{stats['current_memory_usage']:.0f}%",
                                'è¿›åº¦': f"{stats['progress_percent']:.1f}%"
                            })
                            last_stats_time = current_time

                            # è°ƒç”¨è¿›åº¦å›è°ƒå‡½æ•°
                            if progress_callback:
                                progress_info = {
                                    'current': stats['processed_files'],
                                    'total': stats['total_files'],
                                    'percentage': stats['progress_percent'],
                                    'speed': f"{stats['files_per_second']:.2f} æ–‡ä»¶/ç§’",
                                    'eta': f"{stats['eta_seconds']:.0f}ç§’" if stats['eta_seconds'] > 0 else "",
                                    'resource_usage': {
                                        'cpu': f"{stats['current_cpu_usage']:.0f}%",
                                        'memory': f"{stats['current_memory_usage']:.0f}%"
                                    }
                                }
                                progress_callback("Transcribing Audio",
                                                 f"ASRå¤„ç†ä¸­: {stats['processed_files']}/{stats['total_files']} (é€Ÿåº¦: {stats['files_per_second']:.2f}/ç§’)",
                                                 None, progress_info)

                    except Exception as e:
                        logging.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                        # æ·»åŠ ç©ºç»“æœä¿æŒä¸€è‡´æ€§
                        task_index = future_to_index[future]
                        audio_file, seg_index, _ = task_args[task_index]
                        all_results.append((audio_file, seg_index, ""))
                        pbar.update(1)

        # æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - start_time
        final_stats = self.performance_monitor.get_current_stats()

        print(f"\nğŸ‰ EPYCå¹¶è¡Œè½¬å½•å®Œæˆ!")
        print(f"   ğŸ“ å¤„ç†æ–‡ä»¶æ•°: {len(audio_files)}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"   ğŸš€ å¹³å‡é€Ÿåº¦: {final_stats['files_per_second']:.2f}æ–‡ä»¶/ç§’")
        print(f"   ğŸ”¥ å³°å€¼CPUä½¿ç”¨ç‡: {max(final_stats.get('cpu_usage_history', [0])):.0f}%")
        print(f"   ğŸ’¾ å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {final_stats['avg_memory_usage']:.0f}%")
        print(f"   ğŸ“Š æ¨¡å‹ç¼“å­˜ç»Ÿè®¡: {self.model_cache.get_cache_stats()}")

        return all_results

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "num_models": self.num_models,
            "batch_size": self.batch_size,
            "cache_stats": self.model_cache.get_cache_stats(),
            "environment": {
                "OMP_NUM_THREADS": os.environ.get('OMP_NUM_THREADS'),
                "MKL_NUM_THREADS": os.environ.get('MKL_NUM_THREADS'),
                "OPENBLAS_NUM_THREADS": os.environ.get('OPENBLAS_NUM_THREADS')
            }
        }

# å…¨å±€EPYCç®¡ç†å™¨å®ä¾‹
_epyc_manager = None

def get_epyc_manager(config_override: Dict[str, Any] = None) -> EPYCWhisperManager:
    """è·å–å…¨å±€EPYC Whisperç®¡ç†å™¨å®ä¾‹"""
    global _epyc_manager
    if _epyc_manager is None:
        _epyc_manager = EPYCWhisperManager(config_override=config_override)
    return _epyc_manager

def speech_to_text_epyc_optimized(video_name: str, working_dir: str, segment_index2name: Dict[str, str], audio_output_format: str, session_id: str = None, progress_callback=None) -> Dict[str, str]:
    """
    EPYC 64æ ¸å¿ƒä¼˜åŒ–çš„è¯­éŸ³è¯†åˆ«å‡½æ•°

    Args:
        video_name: è§†é¢‘åç§°
        working_dir: å·¥ä½œç›®å½•
        segment_index2name: ç‰‡æ®µç´¢å¼•åˆ°åç§°çš„æ˜ å°„
        audio_output_format: éŸ³é¢‘è¾“å‡ºæ ¼å¼
        session_id: ä¼šè¯IDï¼Œç”¨äºä¸­é—´æ–‡ä»¶å­˜å‚¨

    Returns:
        è½¬å½•ç»“æœå­—å…¸ {segment_index: transcript}
    """
    print(f"ğŸš€ å¼€å§‹EPYCä¼˜åŒ–è¯­éŸ³è¯†åˆ«: {video_name}")
    start_time = time.time()

    # åˆå§‹åŒ–ä¸­é—´æ–‡ä»¶å­˜å‚¨ç®¡ç†å™¨
    storage_manager = None
    if session_id:
        try:
            storage_manager = IntermediateStorageManager(session_id, working_dir)
            storage_manager.append_to_log("03_asr_transcription", f"å¼€å§‹EPYCä¼˜åŒ–è¯­éŸ³è¯†åˆ«: {video_name}")
        except Exception as e:
            logging.warning(f"æ— æ³•åˆå§‹åŒ–ä¸­é—´æ–‡ä»¶å­˜å‚¨ç®¡ç†å™¨: {e}")

    cache_path = os.path.join(working_dir, '_cache', video_name)

    # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    segment_indices = []
    valid_files = 0
    missing_files = 0

    for index in segment_index2name:
        segment_name = segment_index2name[index]
        audio_file = os.path.join(cache_path, f"{segment_name}.{audio_output_format}")

        if os.path.exists(audio_file):
            audio_files.append((audio_file, index))
            segment_indices.append(index)
            valid_files += 1
        else:
            missing_files += 1
            logging.warning(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")

    if storage_manager:
        storage_manager.append_to_log("03_asr_transcription",
            f"æ–‡ä»¶æ£€æŸ¥å®Œæˆ: {valid_files} ä¸ªæœ‰æ•ˆæ–‡ä»¶, {missing_files} ä¸ªç¼ºå¤±æ–‡ä»¶")

    if not audio_files:
        if storage_manager:
            storage_manager.append_to_log("03_asr_transcription", "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶", "ERROR")
            storage_manager.save_error_log({
                "step": "03_asr_transcription",
                "error": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶",
                "video_name": video_name,
                "expected_files": len(segment_index2name),
                "missing_files": missing_files
            })
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶")
        return {}

    print(f"ğŸ“ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶å¾…å¤„ç†")

    # ä¿å­˜ASRé…ç½®
    if storage_manager:
        config = {
            "video_name": video_name,
            "audio_output_format": audio_output_format,
            "total_files": len(audio_files),
            "cache_path": cache_path,
            "use_epyc_optimization": True
        }
        storage_manager.save_step_config("03_asr_transcription", config)

    # è·å–EPYCç®¡ç†å™¨å¹¶å¹¶è¡Œå¤„ç†
    try:
        manager = get_epyc_manager()
        results = manager.parallel_transcribe(audio_files, progress_callback)

        if storage_manager:
            storage_manager.append_to_log("03_asr_transcription",
                f"EPYCå¹¶è¡Œå¤„ç†å®Œæˆï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")

    except Exception as e:
        if storage_manager:
            storage_manager.append_to_log("03_asr_transcription",
                f"EPYCå¤„ç†å¤±è´¥: {str(e)}", "ERROR")
            storage_manager.save_error_log({
                "step": "03_asr_transcription",
                "error": str(e),
                "video_name": video_name,
                "files_processed": 0
            })
        raise e

    # æ„å»ºè¿”å›ç»“æœå¹¶å®æ—¶ä¿å­˜
    transcripts = {}
    successful_transcriptions = 0
    failed_transcriptions = 0

    for i, (audio_file, seg_index, result) in enumerate(results):
        transcripts[seg_index] = result

        if result and result.strip():
            successful_transcriptions += 1
        else:
            failed_transcriptions += 1

        # å®æ—¶ä¿å­˜æ¯ä¸ªç‰‡æ®µçš„è½¬å½•ç»“æœ
        if storage_manager:
            segment_data = {
                "segment_id": seg_index,
                "audio_file": os.path.basename(audio_file),
                "transcript": result,
                "timestamp": time.time(),
                "success": bool(result and result.strip())
            }

            step_path = storage_manager._get_step_path("03_asr_transcription")
            segment_file = step_path / "transcripts_by_segment" / f"segment_{seg_index.zfill(3)}.json"
            storage_manager._atomic_write(segment_file, segment_data)

            # æ¯10ä¸ªç‰‡æ®µè®°å½•ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0 and storage_manager:
                storage_manager.append_to_log("03_asr_transcription",
                    f"å·²å¤„ç† {i + 1}/{len(results)} ä¸ªç‰‡æ®µ")

    # æ€§èƒ½ç»Ÿè®¡å’Œç»“æœä¿å­˜
    total_time = time.time() - start_time
    avg_time_per_file = total_time / len(audio_files) if audio_files else 0

    # è·å–EPYCæ€§èƒ½ç»Ÿè®¡
    performance_stats = manager.get_performance_stats() if 'manager' in locals() else {}

    print(f"âœ… EPYCä¼˜åŒ–è¯­éŸ³è¯†åˆ«å®Œæˆ:")
    print(f"   - å¤„ç†æ–‡ä»¶æ•°: {len(audio_files)}")
    print(f"   - æˆåŠŸè½¬å½•: {successful_transcriptions}")
    print(f"   - å¤±è´¥è½¬å½•: {failed_transcriptions}")
    print(f"   - æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"   - å¹³å‡æ¯æ–‡ä»¶: {avg_time_per_file:.2f}ç§’")
    print(f"   - å¤„ç†é€Ÿåº¦: {len(audio_files)/total_time:.2f}æ–‡ä»¶/ç§’")

    # ä¿å­˜å®Œæ•´ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
    if storage_manager:
        # ä¿å­˜è½¬å½•ç»“æœ
        data = {
            "video_name": video_name,
            "transcripts": transcripts,
            "total_segments": len(segment_index2name),
            "processed_segments": len(audio_files),
            "successful_transcriptions": successful_transcriptions,
            "failed_transcriptions": failed_transcriptions,
            "success_rate": successful_transcriptions / len(audio_files) if audio_files else 0
        }
        storage_manager.save_step_result("03_asr_transcription", data)

        # ä¿å­˜æ€§èƒ½ç»Ÿè®¡
        stats = {
            "video_name": video_name,
            "total_files": len(audio_files),
            "successful_transcriptions": successful_transcriptions,
            "failed_transcriptions": failed_transcriptions,
            "processing_time": total_time,
            "avg_time_per_file": avg_time_per_file,
            "files_per_second": len(audio_files) / total_time if total_time > 0 else 0,
            "performance_stats": performance_stats
        }
        storage_manager.save_step_stats("03_asr_transcription", stats)

        storage_manager.append_to_log("03_asr_transcription",
            f"ASRè½¬å½•å®Œæˆ: {successful_transcriptions}/{len(audio_files)} æˆåŠŸ, è€—æ—¶ {total_time:.2f}s")

    return transcripts